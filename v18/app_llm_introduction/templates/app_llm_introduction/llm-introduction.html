<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    {% load static %}
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>大型語言模型與文字分類技術指南</title>
    <link rel="stylesheet" href="{% static 'assets_llm/css/styles.css' %}" />
  </head>
  <body>
    <div class="container">
      <header class="header">
        <h1>大型語言模型與文字分類技術指南</h1>
        <p class="subtitle">從基礎概念到實作應用的完整教學</p>
      </header>

      <nav class="table-of-contents">
        <h2>目錄</h2>
        <ul>
          <li><a href="#text-classification">1. 文字分類概述</a></li>
          <li><a href="#training-methods">2. 模型訓練方法</a></li>
          <li><a href="#why-custom-models">3. 自訓練模型的必要性</a></li>
          <li><a href="#mlp">4. 多層感知器 (MLP)</a></li>
          <li><a href="#transformer">5. Transformer 模型</a></li>
          <li><a href="#gpt-classification">6. GPT 轉換為分類模型</a></li>
          <li><a href="#model-differences">7. 生成模型 vs 分類模型</a></li>
        </ul>
      </nav>

      <main class="content">
        <section id="text-classification" class="section">
          <h2>1. 文字分類（Text Classification）</h2>

          <div class="definition-box">
            <p>
              <strong>文字分類</strong
              >是自然語言處理（NLP）中的一項核心技術，其目的是將文本自動歸類到一個或多個預先定義的類別中。簡單來說，就是讓電腦自動判斷「這段文字屬於哪一類」。
            </p>
          </div>

          <h3>📌 主要應用場景</h3>
          <div class="applications-grid">
            <div class="app-card">
              <h4>🛡️ 垃圾郵件偵測</h4>
              <p>自動識別並過濾垃圾郵件，保護用戶免受不必要的干擾。</p>
            </div>
            <div class="app-card">
              <h4>😊 情緒分析</h4>
              <p>分析用戶評論、社群媒體貼文的情感傾向（正面、負面、中立）。</p>
            </div>
            <div class="app-card">
              <h4>📰 新聞分類</h4>
              <p>將新聞文章自動分類為財經、體育、科技、政治等不同領域。</p>
            </div>
            <div class="app-card">
              <h4>🎧 客服自動分流</h4>
              <p>根據客戶問題內容，自動分配到相應部門處理。</p>
            </div>
            <div class="app-card">
              <h4>🔍 內容審查</h4>
              <p>自動檢測社群平台上的不當內容，如仇恨言論、暴力內容等。</p>
            </div>
            <div class="app-card">
              <h4>⚖️ 專業文檔分類</h4>
              <p>在法律、醫療等領域，將專業文檔歸類到特定主題或案例類型。</p>
            </div>
          </div>
        </section>

        <section id="training-methods" class="section">
          <h2>2. 模型訓練方法</h2>

          <h3>💡 方法一：利用現成資源</h3>
          <div class="method-box">
            <h4>使用大型語言模型</h4>
            <ul>
              <li>
                <strong>Zero-shot 學習：</strong
                >無需額外訓練數據，直接使用模型進行分類
              </li>
              <li>
                <strong>Few-shot 學習：</strong>提供少量範例，提升分類效果
              </li>
              <li><strong>適用場景：</strong>新聞分類等一般性任務效果良好</li>
              <li>
                <strong>限制：</strong>對特殊分類（如兩岸關係）效果可能不佳
              </li>
            </ul>

            <h4>使用 Hugging Face 預訓練模型</h4>
            <ul>
              <li>豐富的 BERT-based 分類模型</li>
              <li>快速部署，無需從零開始</li>
              <li>支援多種語言和領域</li>
            </ul>
          </div>

          <h3>🔧 方法二：自主訓練模型</h3>
          <div class="training-comparison">
            <div class="training-method">
              <h4>從零開始訓練</h4>
              <div class="pros-cons">
                <div class="cons">
                  <h5>傳統方法（MLP, CNN, LSTM）</h5>
                  <p>❌ 模型能力有限，準確度不佳</p>
                </div>
                <div class="cons">
                  <h5>進階 Transformer 模型</h5>
                  <p>❌ 需要大量數據，資源需求高</p>
                </div>
              </div>
            </div>

            <div class="training-method">
              <h4>遷移學習與微調</h4>
              <p class="method-desc">
                利用預訓練的 Transformer 模型，添加自定義分類層
              </p>

              <div class="model-types">
                <div class="model-type">
                  <h5>BERT 為基礎</h5>
                  <p>雙向編碼器，適合理解任務</p>
                </div>
                <div class="model-type">
                  <h5>GPT 為基礎</h5>
                  <p>單向解碼器，適合生成任務</p>
                </div>
                <div class="model-type">
                  <h5>T5 為基礎</h5>
                  <p>編碼器-解碼器架構，通用性強</p>
                </div>
              </div>

              <div class="training-strategies">
                <div class="strategy">
                  <h5>🔥 全參數微調</h5>
                  <p>✅ 準確率高　❌ 訓練時間長</p>
                </div>
                <div class="strategy">
                  <h5>❄️ 部分參數凍結</h5>
                  <p>
                    ✅
                    訓練快速　若只利用最終隱藏層輸出再接分類輸出層，準確率可能不佳。
                  </p>
                  <p>
                    ✅ 客製化輸出層　
                    利用多層的hidden-states客製化輸出層，可以大幅度提升特定任務的準確率表現。
                  </p>
                </div>
                <div class="strategy">
                  <h5>⚡ LoRA 微調</h5>
                  <p>
                    ✅
                    訓練快速且準確率高，凍結核心基礎模型的權重，在每個區塊的輸出層添加輕量級的適應層Addapter，只有訓練適應層的權重。
                  </p>
                </div>
              </div>
            </div>
          </div>
        </section>

        <section id="why-custom-models" class="section">
          <h2>3. 為什麼要訓練自己的模型？</h2>

          <div class="reasons-grid">
            <div class="reason-card">
              <h3>🔒 安全隱私保護</h3>
              <p>保護敏感數據和智慧財產權，避免資料外洩風險。</p>
            </div>
            <div class="reason-card">
              <h3>💰 成本效益考量</h3>
              <p>避免大型語言模型的高額使用費用和能源消耗。</p>
            </div>
            <div class="reason-card">
              <h3>🎯 專業化優勢</h3>
              <p>針對特定領域任務優化，比通用模型更精準。</p>
            </div>
            <div class="reason-card">
              <h3>⚡ 效能優化</h3>
              <p>輕量化模型，響應速度快，部署成本低。</p>
            </div>
          </div>
        </section>

        <section id="mlp" class="section">
          <h2>4. 多層感知器（MLP）</h2>

          <div class="concept-box">
            <h3>什麼是多層感知器？</h3>
            <p>
              多層感知器是最簡單的前饋神經網絡形式，資訊單向流動，從輸入層到輸出層。它由具有非線性激活函數的全連接神經元組成，廣泛用於處理非線性可分的數據。
            </p>
          </div>

          <div class="architecture-description">
            <h3>MLP 架構特點</h3>
            <ul>
              <li><strong>前饋結構：</strong>無循環或回路，資訊單向傳播</li>
              <li><strong>全連接層：</strong>每層的所有神經元都與下一層連接</li>
              <li>
                <strong>非線性激活：</strong>使用 ReLU、Sigmoid 等激活函數
              </li>
              <li><strong>適用場景：</strong>基礎分類任務、特徵學習</li>
            </ul>
          </div>

          <div class="image-placeholder">
            <img
              src="{% static 'assets_llm/images/An-architecture-of-Multilayer-Perceptron-MLP-58.png' %}"
              alt="MLP 多層感知器架構圖"
              class="architecture-img"
            />
            <p class="image-caption">
              圖：多層感知器的網絡架構示意圖<br />
              <small
                >圖片來源:
                <a
                  href="https://www.researchgate.net/profile/Jamshed-Memon/publication/343273822/figure/fig3/AS:924228077486082@1597364553788/An-architecture-of-Multilayer-Perceptron-MLP-58.png"
                  target="_blank"
                  >ResearchGate</a
                ></small
              >
            </p>
          </div>
        </section>

        <section id="transformer" class="section">
          <h2>5. Transformer 變壓器模型</h2>

          <div class="concept-box">
            <h3>Transformer 革命性創新</h3>
            <p>
              Transformer 是現代 NLP
              的基石，引入了自注意力機制（Self-Attention），能夠並行處理序列數據，大幅提升了模型效能和訓練效率。
            </p>
          </div>

          <h3>🔍 注意力機制（QKV）</h3>
          <div class="attention-explanation">
            <div class="qkv-components">
              <div class="component">
                <h4>Query (Q)</h4>
                <p>查詢向量，代表當前關注的位置</p>
              </div>
              <div class="component">
                <h4>Key (K)</h4>
                <p>鍵向量，用於計算相似度</p>
              </div>
              <div class="component">
                <h4>Value (V)</h4>
                <p>值向量，包含實際的資訊內容</p>
              </div>
            </div>

            <div class="attention-formula">
              <h4>注意力計算公式</h4>
              <div class="formula">
                Attention(Q,K,V) = softmax(QK<sup>T</sup>/√d<sub>k</sub>)V
              </div>
            </div>
          </div>

          <div class="image-placeholder">
            <img
              src="{% static 'assets_llm/images/transformer-architecture.png' %}"
              alt="Transformer 架構圖"
              class="architecture-img"
            />
            <p class="image-caption">
              圖：Transformer 模型的完整架構<br />
              <small
                >圖片來源:
                <a
                  href="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vrSX_Ku3EmGPyqF_E-2_Vg.png"
                  target="_blank"
                  >Medium</a
                ></small
              >
            </p>
          </div>
          <div class="image-placeholder">
            <img
              src="{% static 'assets_llm/images/gpt-architecture.webp' %}"
              alt="GPT 架構圖"
              class="architecture-img"
            />
            <p class="image-caption">
              圖：GPT 模型的完整架構<br />
              <small
                >圖片來源:
                <a
                  href="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*Ez3XmeCXcEgFYtm-DgzmZg.png"
                  target="_blank"
                  >Medium</a
                ></small
              >
            </p>
          </div>
          <div class="image-placeholder">
            <img
              src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*tt5LyFWKChKROiZ6f3ks_Q.png"
              alt="GPT 架構圖"
              class="architecture-img"
            />
            <p class="image-caption">
              圖：GPT 模型的完整架構(類似MLP加上注意力)<br />
              <small
                >圖片來源:
                <a
                  href="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*tt5LyFWKChKROiZ6f3ks_Q.png"
                  target="_blank"
                  >Medium</a
                ></small
              >
            </p>
          </div>
          <div class="image-placeholder">
            <img
              src="https://towardsdatascience.com/wp-content/uploads/2022/09/1Uya6_ec79IIXOkvPbGOQ-g-768x429.png"
              alt="GPT 注意力機制"
              class="architecture-img"
            />
            <p class="image-caption">
              圖：GPT 模型的注意力機制<br />
              <small
                >圖片來源:
                <a
                  href="https://towardsdatascience.com/wp-content/uploads/2022/09/1Uya6_ec79IIXOkvPbGOQ-g-768x429.png"
                  target="_blank"
                  >Towards Data Science</a
                ></small
              >
            </p>
          </div>
          <div class="image-placeholder">
            <img
              src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f41a56a-8b4a-4fbb-9124-82c01f95fc35_3399x1665.png"
              alt="GPT 注意力機制"
              class="architecture-img"
            />
            <p class="image-caption">
              圖：GPT 模型的注意力機制<br />
              After a softmax transformation, this matrix of interactions is called the attention matrix. The resulting output hidden states of the attention layer are the matrix multiplication of the attention matrix and the values vectors.
              經過softmax變換後，這個交互矩陣被稱為注意力矩陣。注意力層最終的輸出隱藏狀態是注意力矩陣和值向量的矩陣乘法。
              <br />
              <small
                >圖片來源:
                <a
                  href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f41a56a-8b4a-4fbb-9124-82c01f95fc35_3399x1665.png"
                  target="_blank"
                  >Towards Data Science</a
                ></small
              >
            </p>
          </div>

          <div class="transformer-advantages">
            <h3>Transformer 優勢</h3>
            <ul>
              <li><strong>並行計算：</strong>可同時處理整個序列，訓練效率高</li>
              <li><strong>長距離依賴：</strong>有效捕捉文本中的長距離關聯</li>
              <li><strong>可解釋性：</strong>注意力權重提供模型決策的可視化</li>
              <li><strong>遷移能力：</strong>預訓練模型可適用於多種下游任務</li>
            </ul>
          </div>
        </section>

        <section id="gpt-classification" class="section">
          <h2>6. GPT 模型轉換為分類模型</h2>

          <h3>🔄 遷移學習過程</h3>
          <div class="transfer-process">
            <div class="step">
              <h4>步驟 1：預訓練架構分析</h4>
              <div class="architecture-components">
                <div class="component-detail">
                  <h5>輸入嵌入層</h5>
                  <ul>
                    <li>Token 嵌入：詞彙表中每個詞的向量表示</li>
                    <li>位置嵌入：序列中每個位置的資訊</li>
                  </ul>
                </div>
                <div class="component-detail">
                  <h5>Transformer 編碼層</h5>
                  <ul>
                    <li>自注意力機制：捕捉上下文關係</li>
                    <li>前饋層：非線性特徵轉換</li>
                    <li>Layer Norm & Dropout：穩定訓練</li>
                  </ul>
                </div>
              </div>
            </div>

            <div class="step">
              <h4>步驟 2：模型改造</h4>
              <div class="modification-details">
                <div class="modification">
                  <h5>🎯 序列表示提取</h5>
                  <p>選擇最後一個 token 的隱藏狀態或使用池化方法</p>
                </div>
                <div class="modification">
                  <h5>🔗 添加分類頭</h5>
                  <p>替換輸出層為全連接層 + Softmax</p>
                </div>
              </div>
            </div>

            <div class="step">
              <h4>步驟 3：訓練策略</h4>
              <div class="training-decisions">
                <div class="decision">
                  <h5>全模型微調 vs 僅訓練分類頭</h5>
                  <div class="comparison">
                    <div class="option">
                      <strong>全模型微調</strong>
                      <p>✅ 效果更好　❌ 資源需求高</p>
                    </div>
                    <div class="option">
                      <strong>僅分類頭</strong>
                      <p>✅ 快速訓練　❌ 效果有限</p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <div class="code-example">
            <h3>💻 實作範例</h3>
            <pre><code>from transformers import AutoModelForSequenceClassification

# 載入預訓練的 GPT 模型
model_name = "gpt2"
num_labels = 5  # 分類類別數

# 將 GPT 模型轉換為分類模型
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels
)</code></pre>
          </div>

          <div class="lora-section">
            <h3>⚡ LoRA 高效微調</h3>
            <div class="lora-explanation">
              <p>
                <strong>LoRA (Low-Rank Adaptation)</strong>
                是一種參數高效的微調方法：
              </p>
              <ul>
                <li>凍結主模型參數，僅訓練小型適配器</li>
                <li>大幅減少訓練時間和記憶體需求</li>
                <li>保持接近全模型微調的效果</li>
                <li>支援多任務切換和模型共享</li>
              </ul>
            </div>
          </div>
        </section>

        <section id="model-differences" class="section">
          <h2>7. 生成模型 vs 分類模型的差異</h2>

          <div class="model-comparison">
            <h3>🔄 模型行為轉變</h3>

            <div class="comparison-table">
              <table>
                <thead>
                  <tr>
                    <th>特性</th>
                    <th>生成模型</th>
                    <th>分類模型</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong>輸入處理</strong></td>
                    <td>按序列逐步處理</td>
                    <td>一次性輸入整個序列</td>
                  </tr>
                  <tr>
                    <td><strong>輸出形式</strong></td>
                    <td>逐字生成文本</td>
                    <td>直接輸出類別分佈</td>
                  </tr>
                  <tr>
                    <td><strong>隱藏狀態用途</strong></td>
                    <td>生成下一個 token</td>
                    <td>提取全局語意表示</td>
                  </tr>
                  <tr>
                    <td><strong>典型任務</strong></td>
                    <td>文本生成、對話</td>
                    <td>分類、情感分析</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="hidden-states-explanation">
            <h3>🧠 隱藏狀態向量的角色</h3>
            <div class="states-usage">
              <div class="usage-type">
                <h4>GPT 模型處理方式</h4>
                <p>
                  使用最後一個 token 的隱藏狀態作為整個序列的語意總結，因為 GPT
                  的單向注意力使最後的 token 包含了完整的上下文資訊。
                </p>
              </div>
              <div class="usage-type">
                <h4>BERT 模型處理方式</h4>
                <p>
                  使用特殊的 [CLS] token 隱藏狀態，該 token
                  專門設計用於序列級別的表示任務。
                </p>
              </div>
              <div class="usage-type">
                <h4>池化方法</h4>
                <p>
                  對所有 token
                  的隱藏狀態進行平均池化或最大池化，綜合整個序列的特徵資訊。
                </p>
              </div>
            </div>
          </div>

          <div class="workflow-diagram">
            <h3>🔄 分類模型工作流程</h3>
            <div class="workflow-steps">
              <div class="workflow-step">
                <div class="step-number">1</div>
                <h4>文本輸入</h4>
                <p>整篇文章或句子一次性輸入</p>
              </div>
              <div class="arrow">→</div>
              <div class="workflow-step">
                <div class="step-number">2</div>
                <h4>特徵提取</h4>
                <p>Transformer 層生成隱藏狀態</p>
              </div>
              <div class="arrow">→</div>
              <div class="workflow-step">
                <div class="step-number">3</div>
                <h4>語意聚合</h4>
                <p>提取序列級別的表示向量</p>
              </div>
              <div class="arrow">→</div>
              <div class="workflow-step">
                <div class="step-number">4</div>
                <h4>分類預測</h4>
                <p>通過分類頭輸出類別機率</p>
              </div>
            </div>
          </div>
        </section>
      </main>

      <footer class="footer">
        <p>&copy; 2024 大型語言模型技術指南. 本文檔旨在教育和學習目的。</p>
      </footer>
    </div>
  </body>
</html>
