from django.shortcuts import render
from django.views.decorators.csrf import csrf_exempt
from django.http import JsonResponse

from datetime import datetime, timedelta
import pandas as pd
import math
import re
from collections import Counter

import markdown
import json
import requests
import app_user_keyword.views as userkeyword_views

url = "http://163.18.22.32:11435/api/generate"
# è¨­ç½®é ç¨‹ Ollama æ¨¡å‹çš„åŸºç¤ URL
REMOTE_OLLAMA_URL = "http://163.18.22.32:11435"

model_name = "gemma3:4b"  # é»˜èªæ¨¡å‹åç¨±
# model_name = "qwen2.5:7b"  # é»˜èªæ¨¡å‹åç¨±
#model_name = "deepseek-r1:14b"  # é»˜èªæ¨¡å‹åç¨±
# åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
print(f"æ­£åœ¨é€£æ¥ {REMOTE_OLLAMA_URL} æª¢æŸ¥å¯ç”¨æ¨¡å‹...")
response = requests.get(f"{REMOTE_OLLAMA_URL}/api/tags")
models = response.json()        
print("\nå¯ç”¨çš„æ¨¡å‹:")
available_models = [model['name'] for model in models['models']]
for model in available_models:
    print(f"- {model}")
# æª¢æŸ¥æŒ‡å®šçš„æ¨¡å‹æ˜¯å¦å¯ç”¨
if model_name in available_models:
    print(f"\nâœ… æ¨¡å‹ '{model_name}' å·²å¯ç”¨")

def load_df_data():
    # import and use df from app_user_keyword 
    global df # global variable
    df = userkeyword_views.df

load_df_data()


# For the key association analysis
def home(request):
    return render(request, 'app_user_keyword_summarize/home.html')

# df_query should be global
@csrf_exempt
def api_get_userkey_summarize(request):

    userkey = request.POST.get('userkey')
    cate = request.POST['cate'] # This is an alternative way to get POST data.
    cond = request.POST.get('cond')
    weeks = int(request.POST.get('weeks'))
    key = userkey.split()

    global  df_query # global variable It's not necessary.

    df_query = filter_dataFrame_fullText(key, cond, cate,weeks)
    print(key)
    print(len(df_query))

    if len(df_query) == 0:
        return {'error': 'No results found for the given keywords.'}

    if len(df_query) != 0:  # df_query is not empty
        newslinks = get_title_link_topk(df_query, k=15)
        related_words, clouddata = get_related_word_clouddata(df_query)

    else:
        newslinks = []
        related_words = []
        clouddata = []

    response = {
        'newslinks': newslinks,
        'related_words': related_words,
        'clouddata':clouddata,
        'num_articles': len(df_query),
    }
    
    return JsonResponse(response)


# Searching keywords from "content" column
# Here this function uses df.content column, while filter_dataFrame() uses df.tokens_v2
def filter_dataFrame_fullText(user_keywords, cond, cate, weeks):

    # end date: the date of the latest record of news
    end_date = df.date.max()

    # start date
    start_date = (datetime.strptime(end_date, '%Y-%m-%d').date() -
                  timedelta(weeks=weeks)).strftime('%Y-%m-%d')

    # (1) proceed filtering: a duration of a period of time
    # æœŸé–“æ¢ä»¶
    period_condition = (df.date >= start_date) & (df.date <= end_date)

    # (2) proceed filtering: news category
    # æ–°èé¡åˆ¥æ¢ä»¶
    if (cate == "å…¨éƒ¨"):
        condition = period_condition  # "å…¨éƒ¨"é¡åˆ¥ä¸å¿…éæ¿¾æ–°èç¨®é¡
    else:
        # categoryæ–°èé¡åˆ¥æ¢ä»¶
        condition = period_condition & (df.category == cate)

    # (3) proceed filtering: news category
    # and or æ¢ä»¶
    if (cond == 'and'):
        # query keywords conditionä½¿ç”¨è€…è¼¸å…¥é—œéµå­—æ¢ä»¶and
        condition = condition & df.content.apply(lambda text: all(
            (qk in text) for qk in user_keywords))  # å¯«æ³•:all()
    elif (cond == 'or'):
        # query keywords conditionä½¿ç”¨è€…è¼¸å…¥é—œéµå­—æ¢ä»¶
        condition = condition & df.content.apply(lambda text: any(
            (qk in text) for qk in user_keywords))  # å¯«æ³•:any()
    # condiction is a list of True or False boolean value
    df_query = df[condition]

    return df_query

# get titles and links from k pieces of news 
def get_title_link_topk(df_query, k=25):
    items = []
    for i in range( len(df_query[0:k]) ): # show only 10 news
        category = df_query.iloc[i]['category']
        title = df_query.iloc[i]['title']
        link = df_query.iloc[i]['link']
        photo_link = df_query.iloc[i]['photo_link']
        # if photo_link value is NaN, replace it with empty string 
        if pd.isna(photo_link):
            photo_link=''
        
        item_info = {
            'category': category, 
            'title': title, 
            'link': link, 
            'photo_link': photo_link
        }

        items.append(item_info)
    return items 

def get_title_content(df_query, k=25):
    datas = []
    for i in range( len(df_query[0:k]) ): # show only 10 news
        title = df_query.iloc[i]['title']
        content = df_query.iloc[i]['content']
 
        data_info = { 
            'title': title, 
            'content': content, 
        }

        datas.append(data_info)
    return datas

@csrf_exempt
def api_get_userkey_report(request):
    mode = request.POST['mode']
    result = api_get_userkey_summarize(request)

    print("æ¨¡å¼ï¼š", mode)

    if isinstance(result, dict) and 'error' in result:
        return JsonResponse(result)

    # ç²å–æ–°èè³‡æ–™ï¼ˆé è¨­æœ€å¤š25å‰‡ï¼‰
    news_items = get_title_content(df_query, k=25)
    print(len(news_items))

    all_reports = []  # å„²å­˜å¤šæ®µå›æ‡‰çš„å®¹å™¨

    # ===== æ‘˜è¦æ¨¡å¼ï¼šä¸€æ¬¡è™•ç†æ‰€æœ‰æ–°è =====
    if mode == 'æ‘˜è¦':
        news_text = ""
        for idx, item in enumerate(news_items):
            news_text += f"### æ–°è {idx+1}\n**æ¨™é¡Œï¼š** {item['title']}\n**å…§å®¹ï¼š** {item['content']}\n\n"

        prompt = f'''
è«‹å¹«æˆ‘ç¸½çµä»¥ä¸‹æ–°èå…§å®¹ï¼Œç”Ÿæˆä¸€ç¯‡å°ˆæ¥­çš„å ±å°æ‘˜è¦ï¼ˆç´„500å­—ï¼‰ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä¸¦ç”¨ Markdown èªæ³•æ’ç‰ˆï¼š

{news_text}
        '''

        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": False
        }

        try:
            response = requests.post(url, json=payload, timeout=100)
            result = response.json()
            final_report = result['response']
        except:
            print("Error:", response.status_code, response.text)
            return JsonResponse({'error': 'Failed to generate summary. Please try again later.'})

        return JsonResponse({'report': final_report})

    # ===== åˆ†æ®µæ¨¡å¼ï¼šè™•ç†é—œéµå•ç­” / åˆ†æå‹å ±å° =====
    batch_size = 2
    for i in range(0, len(news_items), batch_size):  # å¾ index 0 é–‹å§‹
        batch = news_items[i:i + batch_size]

        summary_blocks = ""
        news_text = ""
        for idx, item in enumerate(batch):
            news_index = i + idx + 1  # æ­£ç¢ºæ–°èç·¨è™Ÿ
            title = item['title']
            content = item['content']

            summary_blocks += f'''## ğŸ“° æ–°è {news_index}ï¼š{title}
    ### é‡é»æ‘˜è¦
    ä¸€æ®µæ–°èæ‘˜è¦

    '''
            news_text += f"### æ–°è {news_index}\n**æ¨™é¡Œï¼š** {title}\n**å…§å®¹ï¼š** {content}\n\n"

        if mode == 'é—œéµå•ç­”':
            prompt = f'''

    ä¸è¦ç”Ÿæˆæ–°èå…§å®¹åªè¦ä¸‹é¢æ–‡å­—æœ‰è¦æ±‚çš„ï¼Œä¸”å•é¡Œ1å’Œå•é¡Œ2çš„æ ¼å¼è¦ç›¸åŒ
    ä»¥ä¸‹æ˜¯å¹¾å‰‡æ–°èçš„é‡é»æ‘˜è¦ï¼Œè«‹ä½ æ ¹æ“šé€™äº›è³‡è¨Šç”Ÿæˆå…©å€‹é—œéµå•é¡ŒåŠå…¶å°æ‡‰ç­”æ¡ˆï¼Œæ ¼å¼è«‹ç”¨ Markdownï¼Œåƒé€™æ¨£ï¼š

    ---

    ## ğŸ“° æ–°è 1ï¼šæ–°èæ¨™é¡Œ

    ### é‡é»æ‘˜è¦  
    ä¸€æ®µæ–°èæ‘˜è¦

    ### ğŸ” å•é¡Œèˆ‡å›ç­”

    **å•é¡Œ 1** å•é¡Œæ–‡å­—  
     
    **å›ç­”ï¼š** å›ç­”æ–‡å­—

    **å•é¡Œ 2**  å•é¡Œæ–‡å­—  
    
    **å›ç­”ï¼š** å›ç­”æ–‡å­—

    ---

    {summary_blocks}
    {news_text}
        '''


        elif mode == 'åˆ†æå‹å ±å°':
            prompt = f'''
    è«‹å¹«æˆ‘æ·±å…¥åˆ†æä»¥ä¸‹æ–°èå…§å®¹ï¼Œä¸¦é‡å°æ¯å‰‡æ–°èåˆ†åˆ¥æ’°å¯«ä»¥ä¸‹å…§å®¹ï¼Œå­—æ•¸ç´„ 250 å­—ã€‚è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä¸¦ç”¨ Markdown æ’ç‰ˆï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

    ---

    ## ğŸ“° æ–°è Xï¼šæ–°èæ¨™é¡Œ

    ### æ ¸å¿ƒè­°é¡Œ
    ï¼ˆç°¡è¦èªªæ˜æ–°èçš„æ ¸å¿ƒè­°é¡Œï¼‰

    ### åˆ©å®³é—œä¿‚äºº
    - **åˆ©å®³é—œä¿‚äºº A**ï¼šèªªæ˜
    - **åˆ©å®³é—œä¿‚äºº B**ï¼šèªªæ˜
    ï¼ˆè¦–éœ€è¦å¢æ¸›ï¼‰

    ### å¯èƒ½çš„ç™¼å±•è¶¨å‹¢
    - èªªæ˜å¯èƒ½çš„å¾ŒçºŒç™¼å±• 1
    - èªªæ˜å¯èƒ½çš„å¾ŒçºŒç™¼å±• 2

    ### åˆç†æ¨è«–
    - ç¶œåˆæ¨è«–èˆ‡æ½›åœ¨å½±éŸ¿

    ---

    ä»¥ä¸‹æ˜¯æ–°èå…§å®¹ï¼š

    {news_text}
        '''


        else:
            return JsonResponse({'error': 'ä¸æ”¯æ´çš„æ¨¡å¼'})

        # ç™¼é€ API è«‹æ±‚
        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": False
        }

        try:
            response = requests.post(url, json=payload, timeout=100)
            result = response.json()
            all_reports.append(result['response'])
        except:
            print("Error:", response.status_code, response.text)
            return JsonResponse({'error': 'Failed to generate report in segment.'})

    final_report = '\n\n'.join(all_reports)
    return JsonResponse({'report': final_report})



# Get related keywords by counting the top keywords of each news.
# Notice:  do not name function as  "get_related_keys",
# because this name is used in Django
def get_related_word_clouddata(df_query):

    # wf_pairs = get_related_words(df_query)
    # prepare wf pairs 
    counter=Counter()
    for idx in range(len(df_query)):
        pair_dict = dict(eval(df_query.iloc[idx].top_key_freq))
        counter += Counter(pair_dict)
    wf_pairs = counter.most_common(20) #return list format

    # cloud chart data
    # the minimum and maximum frequency of top words
    min_ = wf_pairs[-1][1]  # the last line is smaller
    max_ = wf_pairs[0][1]
    # text size based on the value of word frequency for drawing cloud chart
    textSizeMin = 20
    textSizeMax = 120
    if (min_ != max_):
        max_min_range = max_ - min_

    else:
        max_min_range = len(wf_pairs) # é—œéµè©çš„æ•¸é‡: 20å€‹
        min_ = min_ - 1 # every size is 1 / len(wf_pairs)
    
    # word cloud chart data using proportional scaling
    # æ’é™¤åˆ†æ¯ç‚º0çš„æƒ…æ³
    clouddata = [{'text':w, 'size':int(textSizeMin + (f - min_)/max_min_range * (textSizeMax-textSizeMin))} for w, f in wf_pairs]


    return   wf_pairs, clouddata 

    
print("app_user_keyword_summarize was loaded!")
